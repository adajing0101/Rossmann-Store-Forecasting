---
title: "Group 3 Project_ Rossmann Sales"
author: "Emma"
date: "3/1/2021"
output: html_document
---
##### Prepare the Data 
<li>Format the data</li>
##### Data Exploration
<li>Annual Sales</li>
<li>Monthly Aggregated Sales</li>
<li>Aggregate Days of Week Sales</li>
##### Time Series Analysis
<li>Correlation Heatmap</li>
<li>Spectrum Analysis</li>
<li>Decompositon of the Data</li>
<li>Stationary</li>
##### Models
<li>Differencing and EACF</li>
<li>Auto ARIMA</li>
<li>Comparison between Auto Arima and Bestfit_arima6</li>
<li>Moving Average Smoothing</li>
<li>Holt-Winter Model</li>
<li>TBATS</li>
<li>Fourier Term</li>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(fpp)
library(anytime)
library(lubridate)# for year function
library(dplyr)
library(tseries)
library(fpp)
library(ggplot2)
library(scales) # for pretty breaks plot
library(TSA)
source('/Users/emmali/Library/Mobile Documents/com~apple~CloudDocs/Time Series/TSA_1.2/TSA/R/eacf.R')
setwd('/Users/emmali/Library/Mobile Documents/com~apple~CloudDocs/Time Series/project/rossmann-store-sales/')
train <- read.csv('train.csv')
test <- read.csv('test.csv')
store <- read.csv('store.csv')
head(train)
#check na values 
sum(is.na(train$sales)) # Returns 0 na values so good to go. 
```


# Prepare the Data 

Summary: 

we have 1115 stores, 4 store type(a,b,c,d) and 3 Assortment  # Store 3 

```{r}
summary(store)
#store[which(store$StoreType =='a'),]
#train[which(train$Open ==0),]
#train[which(train$SchoolHoliday==1),]
#train[which(train$Sales ==0),]
#summary(train)
#sales_total<- aggregate(Sales~Date,  train, FUN = mean) # we could use mean or sum #####
# check data point for each store

train %>% count(Store,sort = TRUE)

#unique(train[which(train$Open==0),]$DayOfWeek)
#unique(train[which(train$DayOfWeek==7),]$Sales)
#temp = train %>% filter(Store==8) %>% arrange(Date)
#unique(temp[which(temp$DayOfWeek==7),]$Sales)

#unique(train[which(train$StateHoliday==1),]$Sales)
#unique(train[which(train$SchoolHoliday==1),]$Sales)


```

#### Choose store 8 as sample

possible store chosen
criteria: no close during Holiday

```{r}
#temp = train %>% filter(Store==85) %>% arrange(Date)
#temp[which(temp$DayOfWeek==7),]$Sales
##unique(temp$Open)
#temp_ts<-ts(temp, f = 7)
#temp_ts
#autoplot(temp_ts[,4])
#tsdisplay(temp_ts[,4])
```

## Format the data


```{r}
data <- read.csv('train.csv')
store_8<-data %>% filter(Store==8) %>% arrange(Date)

# Convert Date column to type "Date", get the first day of our data
store_8$Date = as.Date(store_8$Date, format = "%Y-%m-%d")

# Convert "03-01" to day of the year 
dayOfYear = as.numeric(format(store_8[1,3], "%j"))

#keep the date information for future use  
store_8$Date = anydate(store_8$Date)
store_8$year = year(store_8$Date)
store_8$month = month(store_8$Date)
store_8$week <- week(store_8$Date)
store_8$day = day(store_8$Date)

# double check 365 dates each year without missing 
store_8 %>% count(year,sort = TRUE)

#take a look at the data: no missing data 
summary(store_8)
 
#drop store number 
store_8 <- subset(store_8, select=-1)
# adjust the col sequence 
store_8 <- subset(store_8,select= c(2:ncol(store_8),1))

```

# Data Exploration

```{r}
hist(store_8$Sales, xlab="Store_8 Sales", main = 'Histogram of Rossmann Store 8 Sales (2013-2015)')
```

From the histogram we could see there is large portion of the 0 value which means it is the store closed due to some reasons. 

## Annual Sales 
```{r}

title = 'Store 8 - Annual Sales - 2013 -2015 (Sampled Daily)'
ym_box = ggplot(store_8, aes(x = year, y = Sales, color = factor(sort(year)))) + geom_boxplot()
print(ym_box + ggtitle(title) + labs(y = 'Sales', x = 'Year', color ='Year') + scale_x_continuous(breaks = pretty_breaks())+ scale_y_continuous(breaks = pretty_breaks() ))
      
```

## Monthly Aggregate Sales
```{r}
title = 'Store 8  - Monthly Sales by Year (Sampled Daily)'
ym = subset(store_8, select =c('year','month','Sales'))
ym = aggregate(Sales~year+month, ym, FUN = sum)
ym_line = ggplot(ym, aes(x = month, y =Sales, color = as.character(year))) + geom_line()
print(ym_line + ggtitle(title) + labs(y = 'Sales', x = 'Month', color='Year') + scale_x_continuous(breaks = seq(1, 12, by = 1)))

```
 
From monthly plot, there is a trend from 2013-2015.  and it looks End of Year, the sales will go up and considering the flu season, it makes sense. in the future works, if we could have more data, we could including this monthly seasonality. 

## Weekly Aggreageted  Sales 

```{r}

title = 'Store 8  - Weekly Sales by Year (Sampled Daily)'
ym = subset(store_8, select = c('year',  'week', 'Sales'))
ym = aggregate(Sales ~ year +  week, ym, FUN = mean)
ym_line= ggplot(ym, aes(x = week, y = Sales, color = as.character(year))) + geom_line()
print(ym_line + ggtitle(title)+labs(y = 'Aggregated Sales', x = 'Week Number', color ='Week Number') +scale_x_continuous(breaks  = pretty_breaks())+ scale_y_continuous(breaks = pretty_breaks()))



```



## Aggregate Days of Week Sales 

```{r}

title = 'Store 8  - Day of Week Sales by Year (Sampled Daily)'
ym = subset(store_8, select =c('week','year','DayOfWeek','Sales'))
ym = aggregate(Sales~year+DayOfWeek, ym, FUN = mean)
ym_line = ggplot(ym, aes(x = DayOfWeek, y =Sales, color = as.character(year))) + geom_line()
print(ym_line + ggtitle(title) + labs(y = 'Sales', x = 'DayOfWeek', color='Year') + scale_x_continuous(breaks = seq(1, 7, by = 1)))


```

Day of week is a important factor 
box plot for variation 


```{r}
title = 'Store 8  - Monthly Sales by Year (Sampled Daily)'
ym = subset(store_8, select =c('year','month','Sales'))
ym = aggregate(Sales~year+month, ym, FUN = mean)
ym_box = ggplot(ym, aes(x = month, y = Sales, color = factor(sort(month)))) + geom_boxplot()
print(ym_box + ggtitle(title) + labs(y = 'Sales', x = 'month', color ='month') + scale_x_continuous(breaks = seq(1, 12, by = 1)))

```


# Time Series Analysis
```{r}

#convert to time series data
store_8_ts <- ts(store_8, start = c(2013, dayOfYear),frequency = 365.25)
tsdisplay(store_8_ts[,2], main = 'Sales of store 8')

#split train test data
train <- window(store_8_ts, start = c(2013, 1), end = c(2015, 23))
test <-window(store_8_ts, start = c(2015, 24))

```

Sales and Opens 

Store closed will lead the sales to 0 

```{r}
par(mfrow = c(3,1))
plot(train[1:365,4], type = 'l', col = 'blue', ylab = 'Open' )
plot(train[1:365,6],type = 'l', col = 'red',ylab='StateHolidays')
plot(train[1:365, 2], type = 'l', col = 'red',ylab='Sales')

```

on stateholiday, the store have 0 sales. 


## Correlation Heatmap 

```{r}
cormat <- round(cor(train[,c(2:7,12)]),2)
head(cormat)
cormat
library(reshape2)
melted_cormat <- melt(cormat)
head(melted_cormat)

ggplot(data = melted_cormat , aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()


```

## Spectrum Analysis

Identify the Seasonality 

```{r}
periodogram(store_8[,2])
temp <-periodogram(store_8[,2])
max_freq <- temp$freq[which.max(temp$spec)]
seasonality = 1/max_freq
seasonality

par(mfrow=c(2,1))
spectrum(store_8[,2])
periodogram(store_8[,2])


```

from above, it compose three fundamental frequency. 0.15 contribute more which is 7. so we will use 7 for our Arima model. 


## Decompositon of the Data 

HW couldn't handle long seasonality such as 365. so let's try to use 7 as it frequency

```{r}
store_8_tsw<-ts(store_8, f=7) 
tail(store_8_tsw,n=21)
train_w<-window(store_8_tsw, start = c(1,1), end = c(132,4))
test_w<-window(store_8_tsw, start = c(132,5) )# test 3 weeks data
```


```{r}
train_w[,2] %>% decompose(type= 'mult') %>% autoplot() + xlab("Week") + ggtitle("Sales of Rossmann Drug Company")
train_w[,2] %>% decompose(type= 'add') %>% autoplot() + xlab("Week") + ggtitle("Sales of Rossmann Drug Company")
train_w[,2] %>% mstl(t.window=13, s.window="periodic", robust=TRUE)  %>% autoplot() + xlab("Week") + ggtitle("Sales of Rossmann Drug Company")

```


## Stationary 

```{r}
#ggseasonplot(train[,2], col = rainbow(15), year.labels = TRUE)
ggAcf(train_w[,2], main='Sales of Rossmann Drug Company', col ='red')
```

 so the data is not stationary. 

```{r}
tsdisplay(store_8[,2], main = 'Sales of Rossmann Drug Company Store 8')
tsdisplay(store_8[1:365,2], main= 'Sales of Rossman Drug Company First 365 DAYS')
tsdisplay(store_8[577:942,2], main= 'Sales of Rossman Drug Company Late 365 DAYS')
#plot(store_8[,2], ylab = 'Sales', xlab = "Year", main = 'Sales of Rossmann Drug Company')
```

ACF shows the data is not stationary. There is a baseline value but couldn't see clear trend. It also has strong seasonality. no trend observed.  The maginitude of the data also remain the same. There are significnat auto correlation at lag 7, and lag 14.  

also, from the first 365 days obsevation to the last 365 days observation, the magnitude of the data shows some increasing. 
and  use auto boxCox lambda choose, it gives 0.657 as our lambda value for weekly frequency. In Conclusion, the variance is not stable. we need box cox transformation to stablize the variance. 

```{r}
BoxCox.lambda(train_w[,2])
```


### KPSS Test for stationarity 

```{r}
kpss.test(train_w[,2])

adf.test(train_w[,2])
```

for kpss test, the null hypothesis is that the data are stationary and non-seasonal. the p value is 0.01 for train data set which is smaller than 0.05 which we could reject null hypothesis which means the data is not stationary and nonseasonal. 
although ADF test gives us small p value which we reject the null hypothesis that the data is stationary.

if we do first order differencing

## Differencing and EACF

```{r}

ndiffs(train_w[,2])
nsdiffs(train_w[,2]) 

kpss.test(diff(train_w[,2],d = 1))
adf.test(diff(train_w[,2],d = 1))



tsdisplay(diff(train_w,d=1)[,2])
tsdisplay(diff(train_w,d=1,lag = 7)[,2])

tsdisplay(diff(diff(train_w,d = 1), d= 1,lag = 7)[,2])

#box_lambda transformation

lbd = BoxCox.lambda(train_w[,2])

tsdisplay(BoxCox(train_w[,2], lambda = 'auto'))


train_t<-BoxCox(train_w[,2], lambda = lbd)
test_t <-BoxCox(test_w[,2], lambda = lbd)

kpss.test(train_t) # non stationary with smaller P 
adf.test(train_t) # trend stationary

tsdisplay(train_t)

# choice 1 seasonal differencing with lag 7 
kpss.test(diff(train_w[,2],d = 1, lag = 7))
adf.test(diff(train_w[,2],d = 1, lag = 7))

train_sdiff <- diff(train_w[,2],d = 1, lag = 7)
test_sdiff <- diff(train_w[,2],d = 1, lag = 7)

tsdisplay(train_sdiff) # acf exponentially decaying or sinusoidal and has a hard cutt- off in the PACF at lag 7 try AR model 
eacf(train_sdiff, ar.max = 7, ma.max = 7)

# possible choice for model ARMA(3,0), ARMA(1,4) ARMA(3,4),ARMA(4,4)  set up 4 as maximum 

#choice 2 first order differencing 
kpss.test(diff(train_t,d = 1))
adf.test(diff(train_t,d = 1))

train_diff <- diff(train_w[,2],d = 1)
test_diff <- diff(train_w[,2],d = 1)

tsdisplay(train_diff)
eacf(train_diff,ar.max = 4, ma.max = 4)

## possible choices for model MA(2), MA(3) ARMA(2,2) ARMA(2,3) ARMA(2,4) ARMA(4,2) ARMA(4,4)

#choice 3  box transform with first order seasonal differencing 

kpss.test(diff(train_t,d = 1, lag = 7))
adf.test(diff(train_t,d = 1, lag = 7))


train_t_diff <- diff(train_t,d = 1, lag = 7)
test_t_diff <- diff(test_t,d = 1, lag = 7)

tsdisplay(train_t_diff) # some extrem value show up, maybe due to the 0 sales 

eacf(train_t_diff,ar.max = 4, ma.max = 4)

##  possible choice models  MA(1), MA(4), ARMA(1,1),  ARMA(1,4), ARMA(4,4)

#choice 4 for first seasonal differencing and first order differencing 

eacf(diff(train_t_diff,d = 1),ar.max = 4, ma.max = 4)

#Another way, we could remove sunday 0 sales and set up frq = 6

store_8_6f <- store_8[which(store_8$DayOfWeek!=7),]

store_8_6f_ts<-ts(store_8_6f, f = 6)

tsdisplay(store_8_6f_ts[,2])

BoxCox.lambda(store_8_6f_ts)

tsdisplay(diff(store_8_6f_ts[,2],d=1)) 
tsdisplay(diff(store_8_6f_ts[,2],d=1, lag = 6)) 


```


If we apply first order differencing on lag 7, KPSS test pvalue is larger than 0.1 we accept null hypothesis that data is stationary, we could not reject the null hypothesis, Acf test p value is smaller than 0.05, we reject the null hypothsis and the data is staionary. 

another way to look at it is our store is closed on 7, so if we remove 0 sales for 7 , will the data look better? 

we tried to use f =6  and removed all the sales of Sunday, but the stationary test and plot didn't have too much difference. since f = 7 is a more general case for other store type. we will go with original data set. 

try different modesl 

```{r}

result = data.frame()
#choice 1 seasonal differecing
bestfit_arima1 <-list(aicc = Inf)
bestrmse1 <- list(rmse= NA)
# choice 2
bestfit_arima2 <-list(aicc = Inf)
bestfit_rmse2 <- list(rmse= NA)
# Choice 3
bestfit_arima3 <-list(aicc = Inf)
bestfit_rmse3 <- list(rmse= NA)

p=q=0
for (p in 0:4 )
{ for (q in 0:4){
  fit <- Arima (train_sdiff, order = c(p,0,q), 
                include.drift = TRUE)
  if (fit$aicc < bestfit_arima1$aicc) 
    bestfit_arima1<-fit
  if (fit$rmse <-  accuracy(bestfit_arima1)[2])
    bestfit_rmse <-accuracy(bestfit_arima1)[2]
    else break;
  }
}

bestfit_arima1$aicc

bestfit_arima1
accuracy(bestfit_arima1)
bestfit_rmse
result= cbind('choic1 with seasonal differencing ARMA(3,4)', bestfit_arima1$aicc,accuracy(bestfit_arima1)[2])
colnames(result) <- c('Model', 'AICC', 'RMSE')


## choice 2 
p=q=0
for (p in 1:4 )
{ for (q in 0:4){
  try(
  fit <- Arima (train_diff, order = c(p,0,q))
  )
  if (fit$aicc < bestfit_arima2$aicc) 
    bestfit_arima2<-fit
    else break;
  }
}

bestfit_arima2
result_temp= cbind('choic2 with first order differencing ARMA(1,3)', bestfit_arima2$aicc,accuracy(bestfit_arima2)[2])
result = rbind(result, result_temp)



## choice 3 

for (p in 0:2 )
{ for (q in 0:2){
    for (P in 0:2){
      for (Q in 0:2){
        try(
        fit <-Arima (train_w[,2], order = c(p,0,q), seasonal = list(order =c(0,1,0), period =7),
                      include.drift = TRUE, lambda = 'auto',method = 'ML')
        ) 
        if (fit$aicc < bestfit_arima3$aicc) 
          bestfit_arima3 <- fit
          else break
        
      }
    }
  }
}

bestfit_arima3
result_temp<- cbind('BoxCox Transform with Seasonal differencing ARMA(0,0,0)(0,1,1)[7]', bestfit_arima3$aicc,accuracy(bestfit_arima3)[2])
result = rbind(result, result_temp)

bestfit_arima_auto<-Arima (train_w[,2], order = c(1,0,0), seasonal = list(order =c(1,1,0), period =7),
                include.drift = TRUE, lambda = 'auto',method = 'ML')
bestfit_arima_auto$aicc
accuracy(bestfit_arima_auto)

result_temp<- cbind('auto arima (1,0,0)(1,1,0)', bestfit_arima_auto$aicc,accuracy(bestfit_arima_auto)[2])
result = rbind(result, result_temp)



#choice 4 for first seasonal differencing and first order differencing 
bestfit_arima4 <-list(aicc = Inf)
bestfit_rmse4 <- list(rmse= NA)
for (p in 0:2 )
{ for (q in 0:2){
    for (P in 0:2){
      for (Q in 0:2){
        try(
        fit <-Arima (train_w[,2], order = c(p,1,q), seasonal = list(order =c(P,1,Q), period =7),
                      include.drift = TRUE, lambda = 'auto',method = 'ML')
        ) 
        if (fit$aicc < bestfit_arima4$aicc) 
          bestfit_arima4 <- fit
          else break
        
      }
    }
  }
}
bestfit_arima4
result_temp<- cbind('BoxCox Transform with first order and Seasonal differencing ARMA(0,1,1)(1,1,2)[7]', bestfit_arima4$aicc,accuracy(bestfit_arima4)[2])
result = rbind(result, result_temp)


#choice 5 for first seasonal differencing and first order differencing 
bestfit_arima5 <-list(aicc = Inf)
bestfit_rmse5 <- list(rmse= NA)
for (p in 0:4 )
{ for (q in 0:4){
    for (P in 0:4){
      for (Q in 0:4){
        try(
        fit <-Arima (train_w[,2], order = c(p,1,q), seasonal = list(order =c(P,0,Q), period =7),
                      include.drift = TRUE, lambda = 'auto',method = 'ML')
        ) 
        if (fit$aicc < bestfit_arima5$aicc) 
          bestfit_arima5 <- fit
          else break
        
      }
    }
  }
}
bestfit_arima5
result_temp<- cbind('BoxCox Transform with first order differencing ARMA(0,1,1)(1,0,1)[7]', bestfit_arima5$aicc,accuracy(bestfit_arima5)[2])
result = rbind(result, result_temp)

bestfit_arima6 <-Arima (train_w[,2], order = c(1,0,1), seasonal = list(order =c(1,1,1), period =7),
                include.drift = TRUE, lambda = 'auto',method = 'ML')

bestfit_arima_auto
bestfit_arima6
result_temp<- cbind('boxcox transform with seasonal differencing ARIMA (1,0,1)(1,1,1)', bestfit_arima6$aicc,accuracy(bestfit_arima6)[2])
result = rbind(result, result_temp)
result


```




## Auto ARIMA 

```{r}

train_w[,2] %>% BoxCox(lambda = 'auto') %>% autoplot()
Arima_auto=auto.arima(train_w[,2], lambda= 'auto', approximation = TRUE, seasonal = TRUE)
summary(Arima_auto)
arimaorder(Arima_auto)
Box.test(Arima_auto$residuals, lag = 14, type = "L")


fc_auto<-forecast(Arima_auto,h = 21,level=c(80,95) )
fc_80 <-forecast(Arima_auto,h = 21,level=80)
fc_95<-forecast(Arima_auto,h = 21,level=95)
autoplot(fc_auto)





```

## Comparison between auto Arima and bestfit_arima6
```{R}

accuracy(bestfit_arima6)
accuracy(Arima_auto)

fc_arima6<-forecast(bestfit_arima6, h = 21,level=c(80,95)) 
autoplot(fc_arima6)
accuracy(fc_auto,test_w[,2])
accuracy(fc_arima6,test_w[,2])

checkresiduals(Arima_auto)
checkresiduals(bestfit_arima6)
Box.test(bestfit_arima6$residuals, lag = 14, type = 'L')
Box.test(bestfit_arima6$residuals, lag = 7, type = 'L')

plot(fc_auto$mean, ylim = c(0,40000), ylab = 'Sales Rossmann Drug Company', col = 2, main = 'Forecast of 3 Weeks Sales')
lines(test_w[,2], col=1, type = 'l')
polygon(c(time(test_w[,2]),rev(time(test_w[,2]))), c(fc_80$upper,rev(fc_80$lower)),  # create %80 interval
   col=rgb(0,0,0.6,0.2), border=FALSE)
polygon(c(time(test_w[,2]),rev(time(test_w[,2]))), c(fc_95$upper,rev(fc_95$lower)),  # create %95 interval
   col=rgb(0,0,0.3,0.2), border=FALSE)
legend('topleft', legend = c('AutoArima Forecasting', 'Original', '80% CI', '95% CI') , col=c(2, 1,rgb(0,0,0.6,0.2),rgb(0,0,0.3,0.2)), lty = 1)


plot(fc_arima6$mean, ylim = c(0,40000), ylab = 'Sales Rossmann Drug Company', col = 2, main = 'Forecast of 3 Weeks Sales')
lines(test_w[,2], col=1, type = 'l')
polygon(c(time(test_w[,2]),rev(time(test_w[,2]))), c(fc_80$upper,rev(fc_80$lower)),  # create %80 interval
   col=rgb(0,0,0.6,0.2), border=FALSE)
polygon(c(time(test_w[,2]),rev(time(test_w[,2]))), c(fc_95$upper,rev(fc_95$lower)),  # create %95 interval
   col=rgb(0,0,0.3,0.2), border=FALSE)
legend('topleft', legend = c('Best Arima Forecasting', 'Original', '80% CI', '95% CI') , col=c(2, 1,rgb(0,0,0.6,0.2),rgb(0,0,0.3,0.2)), lty = 1)


```



## Moving Average Smoothing

```{r}
ma_7<-ma(train_w[,2],7)
ma_14 <-ma(train_w[,2],14) 
ma_28 <-ma(train_w[,2],28) 
ma_42 <-ma(train_w[,2],42) 
ma_56<-ma(train_w[,2],56)

par(mfrow = c(2,2))
plot(train_w[,2], ylab = 'Sales')
points(ma_7, main = '7-MA', type = 'l',col = 2)

plot(train_w[,2], ylab = 'Sales')
points(ma_14, main = '14-MA', type = 'l',col = 2)

plot(train_w[,2], ylab = 'Sales')
points(ma_28, main = '28-MA', type = 'l',col = 2)

plot(train_w[,2], ylab = 'Sales')
points(ma_42, main = '42-MA', type = 'l',col = 2)


par(mfrow = c(3,2))
plot(train_w[,2], main = 'Original', ylab = 'Sales')
plot(ma_7, main = '7-MA')
plot(ma_14, main = '14-MA')
plot(ma_28, main = '28-MA')
plot(ma_42, main = '42-MA')
plot(ma_56, main = '56-MA')




```


 Weighted Moving Average

```{r}

ma_2X7<-ma(train_w[,2],order = 7, centre = TRUE)

ma_2X14 <-ma(train_w[,2],14,centre = TRUE) 
ma_2X28 <-ma(train_w[,2],28,centre = TRUE) 
ma_2X42 <-ma(train_w[,2],42,centre = TRUE) 
ma_2X56<-ma(train_w[,2],56,centre = TRUE)

par(mfrow = c(2,2))
plot(train_w[,2], ylab = 'Sales')
points(ma_2X7, main = '7-MA', type = 'l',col = 2)

plot(train_w[,2], ylab = 'Sales')
points(ma_2X14, main = '14-MA', type = 'l',col = 2)

plot(train_w[,2], ylab = 'Sales')
points(ma_2X28, main = '28-MA', type = 'l',col = 2)

plot(train_w[,2], ylab = 'Sales')
points(ma_2X42, main = '42-MA', type = 'l',col = 2)


par(mfrow = c(3,2))
plot(train_w[,2], main = 'Original', ylab = 'Sales')
plot(ma_2X7, main = '2X7-MA')
plot(ma_2X14, main = '2X14-MA')
plot(ma_2X28, main = '2X28-MA')
plot(ma_2X42, main = '2X42-MA')
plot(ma_2X56, main = '2X56-MA')



```



## Holt-Winter Model

here we are using additive Holt_Winter's Model. Since our data contains 0 values so additive seasonality are used here.  


```{r}
hw_model= hw(train_w[,2],h=21, damped = TRUE, seasonal='additive')
summary(hw_model)
hw_model$model
plot(hw_model)

checkresiduals(hw_model$residuals)
hw_acc<- accuracy(hw_model$mean, test_w[,2])
accuracy(hw_model)

hw_acc

plot(hw_model$mean, ylim = c(0,10000), ylab = 'Sales Rossmann Drug Company', col = 1, main = 'Forecast of 3 weeks Sales')
points(test_w[,2], col='2', type = 'l')
legend ('bottomright',legend = c('Forecasted','Actual'),col =c( 1,2),lty= 1)

hw_model_no_damped= hw(train_w[,2],h=21, damped = FALSE, seasonal='additive')
summary(hw_model_no_damped)
hw_model_no_damped$model
plot(hw_model_no_damped)


checkresiduals(hw_model$residuals)
checkresiduals(hw_model_no_damped$residuals)

hw_acc_no_damped<- accuracy(hw_model_no_damped$mean, test_w[,2])
accuracy(hw_model_no_damped)

```

From the residual, we still coud see some lags have higher autocorrelation. The residual are not pure white noise. 

and from the Aicc and accurancy, the damped hw model slightly better than no damped ones. 

For the smoothing parameter, φ = 0.9018 which approximatedly equal to 1 .This larger φ means the trend line is changing more rapidly. The smaller gamma γ=0.012  means the seasonal component hardly changed over time. and the smaller beta which is e-04 indicates that the slope component hardly changes over time.



## TBATS
TBATS for long time series data. Allow for annual seasonality as well as weekly seasonality. In that case, a multiple seasonal model such as TBATS is useful. For our daily data, we think it might have a weekly pattern as well as an annual patter.  More people come to store on Weekends. 


```{r}
#data$Sale%>% mstl() %>% autoplot() + xlab ('daily') # mstl is for decomposition 
```

transoform the data using multi

942 data points. leave 21 for forecasting. so 921 for training 

```{r}

train_tbats <-store_8[1:920,] # split data for training/testing
test_tbats<-store_8[921:941,]

m_tbats<-tbats(train_tbats[,2],seasonal.period = c(7,365.25))
m_tbats$AIC
Arima_auto$aic

fc_tbats_80 <- forecast(m_tbats, h = 21, level =80)
fc_tbats_95 <- forecast(m_tbats, h = 21, level =95)
fc_tbats_80$mean
plot(fc_tbats_95$mean)

summary(m_tbats)
plot(fc_tbats_80$mean, col = 1, main ='Model TBATS{1, (4,2), -,{<7,3>,<365.25,1>}', ylim = c(0,10000))
points(time(fc_tbats_80$mean),test_tbats[,2], col=2, type ='l')
polygon(c(time(fc_tbats_80$mean),rev(time(fc_tbats_80$mean))), c(fc_tbats_80$upper,rev(fc_tbats_80$lower)),  # create %80 interval
   col=rgb(0,0,0.6,0.2), border=FALSE)
polygon(c(time(fc_tbats_80$mean),rev(time(fc_tbats_80$mean))), c(fc_tbats_95$upper,rev(fc_tbats_95$lower)),  # create %95 interval
   col=rgb(0,0,0.3,0.2), border=FALSE)
legend('topleft', legend = c(' Tbats Forecasting', 'Original', '80% CI', '95% CI') , col=c(2, 1,rgb(0,0,0.6,0.2),rgb(0,0,0.3,0.2)), lty = 1)


accuracy(m_tbats)
accuracy(fc_tbats_80$mean,test_tbats[,2] )

checkresiduals(m_tbats)
m_residuals <-train_tbats[,2]-m_tbats$fitted.values
Box.test(m_residuals, lag = 7, type = 'L')



```

Model TBATS{1, (4,2), -,{<7,3>,<365.25,1>}  AIC: 19554.96   auto arima AIC 5805.132

Model shows 1 as box-cox transformation which means no box-cox transofrmation. {4,2} shows ARIMA error, - no damping parapeter, 
seasonal period = 7, fourier terms = 3, seasonal period = 365.25, terms = 1
p value is small, reject null, residual aren't independent from each other. 


## Fourier Term 
use fourier terms and the arima model by minimizing the AICc

```{r}
library(forecast)
bestfit <-list(aicc = Inf)
for (i in 1:3)
{
  fit = auto.arima(train_w[,2], xreg = fourier(train_w[,2], K = i), seasonal = FALSE)
  if (fit$aicc < bestfit$aicc) 
    bestfit <-fit
    else break;
}

bestfit
fc_f <-forecast(bestfit, xreg = fourier(train_w[,2], K = 3, h =21))
plot(fc_f)




fc_f_80 <- forecast(bestfit, xreg = fourier(train_w[,2], K = 3, h =21), level =80)

fc_f_95 <- forecast(bestfit, xreg = fourier(train_w[,2], K = 3,  h = 21), level =95)
fc_f_80$mean


plot(fc_f_80$mean, col = 1, main ='Model Fourier', ylim = c(0,10000))
points(time(fc_f_80$mean),test_tbats[,2], col=2, type ='l')
polygon(c(time(fc_f_80$mean),rev(time(fc_f_80$mean))), c(fc_f_80$upper,rev(fc_f_80$lower)),  # create %80 interval
   col=rgb(0,0,0.6,0.2), border=FALSE)
polygon(c(time(fc_f_80$mean),rev(time(fc_f_80$mean))), c(fc_f_95$upper,rev(fc_f_95$lower)),  # create %95 interval
   col=rgb(0,0,0.3,0.2), border=FALSE)
legend('topleft', legend = c('Fourier Forecasting', 'Original', '80% CI', '95% CI') , col=c(2, 1,rgb(0,0,0.6,0.2),rgb(0,0,0.3,0.2)), lty = 1)

checkresiduals(bestfit)
accuracy(fc_f)

accuracy(fc_f, test_w[,2])
bestfit
```

The best model have 3 pairs of Fourier terms  with nt process is an Arima (0,1,4).  Total number of degrees of freedom  is 10 ( 3*2 -seasonality, 4 ARMA)






